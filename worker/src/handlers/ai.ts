/**
 * AI Handler Module
 * 
 * Handles AI assistant requests using Cloudflare Workers AI with AI Gateway.
 * Implements the Remote Team Professional persona.
 * 
 * AI Gateway (aw-cf-ai) handles:
 * - Rate limiting
 * - Caching
 * - Analytics
 * - Cost tracking
 */

// Env is defined in worker-configuration.d.ts (generated by wrangler types)

/**
 * AI Gateway Configuration
 */
const AI_GATEWAY_ID = 'aw-cf-ai';

/**
 * System prompt for Remote Team Professional persona
 */
const SYSTEM_PROMPT = `You are an AI assistant specialized in helping remote team professionals manage their communications and work effectively.

Your capabilities include:
- Summarizing long conversation threads to help people catch up quickly
- Extracting action items from team chats so nothing falls through the cracks
- Identifying priority messages that need immediate attention
- Tracking decisions made in conversations for easy reference
- Providing context-aware answers based on conversation history

Be concise, professional, and action-oriented. Focus on helping remote teams stay aligned and productive.`;

/**
 * Available AI models
 */
export const AI_MODELS = {
	LLAMA_8B: '@cf/meta/llama-3.1-8b-instruct',
	LLAMA_70B: '@cf/meta/llama-3.3-70b-instruct-fp8-fast',
} as const;

/**
 * Embedding model for RAG
 */
export const EMBEDDING_MODEL = '@cf/baai/bge-base-en-v1.5';

/**
 * AI request payload
 */
export interface AiRequest {
	query: string;
	conversationContext?: {
		conversationId: string;
		recentMessages?: Array<{
			senderId: string;
			senderName?: string;
			content: string;
			timestamp: string;
		}>;
	};
	model?: keyof typeof AI_MODELS;
	userId?: string; // For gateway metadata
}

/**
 * AI response payload
 */
export interface AiResponse {
	success: boolean;
	response?: string;
	error?: string;
	model?: string;
}

/**
 * Format conversation history for AI context
 */
function formatConversationHistory(
	messages: any
): string {
	if (!messages || messages.length === 0) {
		return 'No previous conversation history available.';
	}

	return messages
		.map((msg: any) => {
			const sender = msg.senderName || msg.senderId;
			const timestamp = new Date(msg.timestamp).toLocaleString();
			return `[${timestamp}] ${sender}: ${msg.content}`;
		})
		.join('\n');
}

/**
 * Build the full prompt with system instructions and context
 */
function buildPrompt(query: string, conversationHistory?: string): string {
	if (!conversationHistory) {
		return query;
	}

	return `Based on the following conversation history, please answer the user's question.

CONVERSATION HISTORY:
${conversationHistory}

USER QUESTION:
${query}

Please provide a helpful, concise answer based on the conversation context above.`;
}

/**
 * Validate AI request payload
 */
function validateAiRequest(body: any): { valid: boolean; error?: string } {
	if (!body) {
		return { valid: false, error: 'Request body is required' };
	}

	if (!body.query || typeof body.query !== 'string') {
		return { valid: false, error: 'Query must be a non-empty string' };
	}

	if (body.query.length > 2000) {
		return { valid: false, error: 'Query must be less than 2000 characters' };
	}

	if (body.query.trim().length === 0) {
		return { valid: false, error: 'Query cannot be empty' };
	}

	return { valid: true };
}

/**
 * Fetch conversation history from Durable Object
 */
async function fetchConversationHistory(
	env: Env,
	conversationId: string,
	limit: number = 50
): Promise<Array<{ senderId: string; content: string; timestamp: string }>> {
	try {
		const doId = env.CONVERSATION.idFromName(conversationId);
		const stub = env.CONVERSATION.get(doId);
		
		const response = await stub.fetch(
			`https://fake-host/messages?conversationId=${conversationId}&limit=${limit}`
		);
		
		if (!response.ok) {
			console.error('Failed to fetch conversation history:', await response.text());
			return [];
		}
		
		const data = await response.json() as { messages: Array<{
			id: string;
			senderId: string;
			content: string;
			createdAt: string;
		}> };
		
		return data.messages.map(msg => ({
			senderId: msg.senderId,
			content: msg.content,
			timestamp: msg.createdAt,
		}));
	} catch (error) {
		console.error('Error fetching conversation history:', error);
		return [];
	}
}

/**
 * Handle AI chat requests
 * Uses AI Gateway for rate limiting, caching, and analytics
 */
export async function handleAiChat(
	request: Request,
	env: Env
): Promise<Response> {
	try {
		// Parse request body
		let body: any;
		try {
			body = await request.json();
		} catch (parseError) {
			return new Response(
				JSON.stringify({ success: false, error: 'Invalid JSON in request body' }),
				{ status: 400, headers: { 'Content-Type': 'application/json' } }
			);
		}

		// Validate request
		const validation = validateAiRequest(body);
		if (!validation.valid) {
			return new Response(
				JSON.stringify({ success: false, error: validation.error }),
				{ status: 400, headers: { 'Content-Type': 'application/json' } }
			);
		}

		const aiRequest = body as AiRequest;

		// Fetch conversation history from DO if conversationId provided
		let recentMessages = aiRequest.conversationContext?.recentMessages || undefined;
		
		if (!recentMessages && aiRequest.conversationContext?.conversationId) {
			try {
				const messages = await fetchConversationHistory(
					env,
					aiRequest.conversationContext.conversationId,
					50 // Fetch last 50 messages for context
				);
				recentMessages = messages;
			} catch (historyError) {
				console.error('Failed to fetch conversation history:', historyError);
				// Continue without history - don't fail the whole request
			}
		}

		// Format conversation history if available
		const conversationHistory = recentMessages
			? formatConversationHistory(recentMessages)
			: undefined;

		// Build the full prompt
		const userPrompt = buildPrompt(aiRequest.query, conversationHistory);

		// Select model (default to faster 8B model)
		const modelName = aiRequest.model ? AI_MODELS[aiRequest.model] : AI_MODELS.LLAMA_8B;

		// Call Workers AI with AI Gateway
		// Gateway handles rate limiting, caching, and analytics
		const aiResponse: any = await (env.AI as any).run(
			modelName,
			{
				messages: [
					{ role: 'system', content: SYSTEM_PROMPT },
					{ role: 'user', content: userPrompt }
				],
				max_tokens: 512,
				temperature: 0.7,
			},
			{
				gateway: {
					id: AI_GATEWAY_ID,
					metadata: {
						conversationId: aiRequest.conversationContext?.conversationId,
						userId: aiRequest.userId,
						model: modelName,
						timestamp: new Date().toISOString(),
					}
				}
			}
		);

		// Extract response text
		const responseText = aiResponse.response || 'No response generated';

		if (typeof responseText !== 'string' || responseText.trim().length === 0) {
			return new Response(
				JSON.stringify({ success: false, error: 'AI generated an empty response' }),
				{ status: 500, headers: { 'Content-Type': 'application/json' } }
			);
		}

		// Return successful response
		const response: AiResponse = {
			success: true,
			response: responseText,
			model: modelName,
		};

		return new Response(JSON.stringify(response), {
			headers: { 'Content-Type': 'application/json' },
		});

	} catch (error) {
		console.error('AI request error:', error);
		
		let errorMessage = 'An unexpected error occurred';
		let statusCode = 500;

		if (error instanceof Error) {
			errorMessage = error.message;
		}

		return new Response(
			JSON.stringify({ success: false, error: errorMessage }),
			{ status: statusCode, headers: { 'Content-Type': 'application/json' } }
		);
	}
}

/**
 * Generate embeddings for text using Workers AI
 */
export async function generateEmbedding(
	env: any,
	text: string,
	metadata?: Record<string, any>
): Promise<number[]> {
	const response: any = await (env.AI as any).run(
		EMBEDDING_MODEL,
		{ text: [text] },
		{
			gateway: {
				id: AI_GATEWAY_ID,
				metadata: {
					...metadata,
					operation: 'embedding',
					timestamp: new Date().toISOString(),
				}
			}
		}
	);

	if (!response.data || !response.data[0]) {
		throw new Error('Failed to generate embedding');
	}

	return response.data[0];
}
